{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d964f9-6d11-4d34-bbaa-f3b9934a62e1",
   "metadata": {},
   "source": [
    "# Q.1 \n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an ensemble of weak learners, typically decision trees, in a sequential manner. Each new tree is trained to correct the errors of the combined ensemble of all previous trees. Gradient Boosting minimizes a loss function by adding weak learners that move in the direction of the negative gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4769f4f-f9ba-4bfc-971a-5ec56bd1bc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.927865970316415e-08\n",
      "R-squared: 0.9999999964870526\n"
     ]
    }
   ],
   "source": [
    "# Q.2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.loss = mean_squared_error\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize residuals as the target values\n",
    "        residuals = y.copy()\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = self._train_weak_learner(X, residuals)\n",
    "            predictions = model.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            self.models.append(model)\n",
    "\n",
    "    def _train_weak_learner(self, X, y):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "    y = np.array([1.2, 1.9, 3.1, 4.0, 5.1, 5.8, 7.1, 8.0, 9.2, 10.1])\n",
    "\n",
    "    # Train the model\n",
    "    model = SimpleGradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1fa0a0-1d32-48ce-b300-44654dd6124c",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6716f7a3-8291-4c01-814a-b2ae14e34d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 100}\n",
      "Mean Squared Error: 1.9497847034658152e-16\n",
      "R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be22db5-a9ba-4483-8e95-c59e4c74d329",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. It is typically a simple model, such as a shallow decision tree. The goal of gradient boosting is to combine these weak learners to create a strong ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59f344-32ad-4bc9-84bc-0a64d1e6778e",
   "metadata": {},
   "source": [
    "# Q.5\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a strong predictive model by sequentially adding weak learners that correct the errors of the existing ensemble. Each new learner is trained to predict the residual errors (gradients) of the combined model from the previous iterations. This process ensures that each subsequent learner improves the overall performance by focusing on the difficult-to-predict examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40a553-11b5-4c08-a281-e6b103947c97",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by iteratively adding new models to correct the errors of the existing ensemble. The process involves:\n",
    "\n",
    "Initialize: Start with an initial model (e.g., predicting the mean of the target variable).\n",
    "Calculate Residuals: Compute the residuals, which are the differences between the actual and predicted values.\n",
    "Fit Weak Learner: Train a weak learner on the residuals.\n",
    "Update Model: Add the new weak learner to the ensemble, scaled by a learning rate.\n",
    "Repeat: Repeat steps 2-4 for a predefined number of iterations or until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fc44a-a43f-402d-935d-600ae72bdf2d",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "Define the Loss Function: Choose a differentiable loss function that measures the difference between the actual and predicted values.\n",
    "Initialize the Model: Start with an initial prediction, typically the mean of the target variable for regression.\n",
    "Compute Residuals: Calculate the residuals as the negative gradient of the loss function with respect to the current model's predictions.\n",
    "Train Weak Learner: Fit a weak learner (e.g., a decision tree) to the residuals.\n",
    "Update Model: Update the model by adding the weak learner's predictions, scaled by a learning rate.\n",
    "Iterate: Repeat steps 3-5 for a predefined number of iterations or until convergence.\n",
    "Final Model: The final model is the sum of the initial prediction and the weighted predictions of all weak learne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91c69a-2485-46f3-a02e-23dcfcaec00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
