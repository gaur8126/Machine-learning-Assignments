{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2333b38-ca18-46e3-8803-889abfdbed77",
   "metadata": {},
   "source": [
    "## Q.1 \n",
    "\n",
    "A decision tree classifier is a supervised learning algorithm used for classification problems. It works by splitting the data into subsets based on the value of input features. This process is recursive and continues until the model has sufficiently classified the data or certain stopping criteria are met.\n",
    "\n",
    "Steps in Decision Tree Algorithm:\n",
    "\n",
    "Select the Best Feature to Split On: At each node, the algorithm chooses the feature that best separates the data according to a criterion like Gini impurity or information gain (based on entropy).\n",
    "Split the Data: Divide the dataset into subsets that contain instances with similar values for the chosen feature.\n",
    "Create Decision Nodes and Leaf Nodes: If the subset is pure (i.e., all instances belong to the same class), a leaf node is created. If not, the process is repeated for the new subsets.\n",
    "Repeat Recursively: This process is repeated recursively for each branch, considering only the remaining features until all data is classified or the maximum depth is reached.\n",
    "Making Predictions:\n",
    "\n",
    "To make a prediction, start at the root of the tree and move down the branches by evaluating the features of the input data. Follow the branch corresponding to the feature value of the instance until a leaf node is reached. The class label of the leaf node is the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e85e8-b773-490a-9be4-c37b3288d621",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "\n",
    "Step by step mathmatical intution behind decision tree - \n",
    "\n",
    "1. Entropy - for impurity check , and it is used for small dataset\n",
    "formula- H(S) = -p1log2p1 - p2log2p2\n",
    "\n",
    "2. Gini Impurity - it is used for large dataset\n",
    "formula = G.I = 1- (summition of)(p**2)\n",
    "\n",
    "3. Informatio gain - it is used for split the node which is splitted first based on their information gain value \n",
    "I.G 0 - H(S) - (summition of )|Sv|/|S| * H(Sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd30a2e-4613-4ac9-9290-a6d26e4108aa",
   "metadata": {},
   "source": [
    "## Q.3\n",
    "\n",
    "In a binary classification problem, the target variable has two classes, say 0 and 1. The decision tree classifier works as follows:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "The algorithm starts at the root node and selects the feature that best splits the data into two classes based on a criterion like Gini impurity or information gain.\n",
    "It recursively splits the dataset into subsets using the chosen feature, creating a tree structure with decision nodes and leaf nodes.\n",
    "Prediction Phase:\n",
    "\n",
    "For a new instance, the model starts at the root node and evaluates the features of the instance.\n",
    "It follows the branches based on feature values until it reaches a leaf node.\n",
    "The class label at the leaf node is the predicted class for the instanc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cbcf7-ceed-41b3-80f3-eef268ef7651",
   "metadata": {},
   "source": [
    "## Q.4 \n",
    "\n",
    "The geometric intuition behind decision tree classification involves visualizing how the data is partitioned in the feature space.\n",
    "\n",
    "Feature Space Partitioning:\n",
    "\n",
    "Each decision node in the tree corresponds to a splitting hyperplane in the feature space. This hyperplane divides the space into regions, each associated with different branches of the tree.\n",
    "For a 2D feature space, each split is a line (or axis-aligned boundary) that divides the plane into two regions.\n",
    "As the tree grows, the feature space is partitioned into smaller and smaller regions.\n",
    "Making Predictions:\n",
    "\n",
    "When making a prediction, the algorithm navigates through these partitions based on the input features until it reaches a region (leaf node) associated with a class label.\n",
    "The path followed in the tree can be seen as traversing through the regions in the feature space until landing in a region corresponding to a specific class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d3ed9-8ffb-4b88-8d5c-169ab24500a1",
   "metadata": {},
   "source": [
    "## Q.5\n",
    "\n",
    "Confusion matrix is table it is used for evaluate the performance of classification model.\n",
    "\n",
    "1. Accuracy = (TP + TN)/(TP+FP+TN+FN)\n",
    "2. Precision = TP/(TP+FP)\n",
    "3. Recall = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa5798-5a58-4f3a-8e86-2280d4b155be",
   "metadata": {},
   "source": [
    "## Q.6\n",
    "\n",
    "EX- \n",
    "\n",
    "TP = 5 , FP = 4, FN = 3, TN = 2\n",
    "\n",
    "1. Accuracy = (5+2)/(5+4+3+2) = 0.5\n",
    "2. Precision = 5/(5+4) = .55\n",
    "3. Recall = 5/(5+3) = .625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adc996-d8f3-4be5-a39b-ca239acacbe6",
   "metadata": {},
   "source": [
    "## Q.7\n",
    "\n",
    "Context of the Problem:\n",
    "\n",
    "In medical diagnostics, false negatives might be more critical than false positives, making recall more important.\n",
    "In spam detection, false positives (legitimate emails marked as spam) might be more problematic, making precision more important.\n",
    "Imbalanced Datasets:\n",
    "\n",
    "For imbalanced datasets, accuracy might be misleading. Metrics like F1 score, precision, and recall are more informative.\n",
    "Business Impact:\n",
    "\n",
    "The chosen metric should align with the business impact. For example, in financial fraud detection, identifying fraudulent transactions (high recall) might be prioritized over the precision of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbc62f-9012-4bec-9c8f-0ca44c1b391e",
   "metadata": {},
   "source": [
    "## Q.8 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
