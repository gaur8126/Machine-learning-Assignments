{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d88e7d-2387-4932-b976-d5de5496f9e4",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. It differs from other clustering techniques in its approach to organizing data points:\n",
    "\n",
    "Hierarchical Nature: Hierarchical clustering creates a tree-like structure (dendrogram) of nested clusters. Unlike partitioning methods like K-means, which require a predefined number of clusters, hierarchical clustering can create a comprehensive hierarchy from which clusters at different levels can be extracted.\n",
    "No Need for Pre-specification of Clusters: Unlike K-means, hierarchical clustering does not require the number of clusters to be specified in advance. The hierarchy can be cut at the desired level to obtain the number of clusters.\n",
    "Two Main Approaches: Hierarchical clustering can be done in two ways: agglomerative (bottom-up) or divisive (top-down)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0e95d-7d4c-4d42-9c07-daf45d0a9b3f",
   "metadata": {},
   "source": [
    "# Q.2 \n",
    "\n",
    "Agglomerative Hierarchical Clustering (AHC):\n",
    "\n",
    "Bottom-Up Approach: Starts with each data point as its own cluster and iteratively merges the closest pairs of clusters until all points are in a single cluster or a specified number of clusters is reached.\n",
    "Process: At each step, the pair of clusters with the smallest distance between them is merged. This continues until a single cluster remains or the desired number of clusters is achieved.\n",
    "Divisive Hierarchical Clustering (DHC):\n",
    "\n",
    "Top-Down Approach: Starts with all data points in a single cluster and iteratively splits clusters into smaller clusters until each data point is its own cluster or a specified number of clusters is reached.\n",
    "Process: At each step, the cluster with the largest distance within it is split into two. This continues until every data point is in its own cluster or the desired number of clusters is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e4133-52d1-451f-8585-31a862040a67",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "Single Linkage (Minimum Linkage): Distance between the nearest points in the two clusters.\n",
    "Complete Linkage (Maximum Linkage): Distance between the farthest points in the two clusters.\n",
    "Average Linkage: Average distance between all pairs of points in the two clusters.\n",
    "Centroid Linkage: Distance between the centroids (mean points) of the two clusters.\n",
    "Ward's Method: Increases in the total within-cluster variance after merging two clusters. Minimizes the variance within clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9d8b9-3118-427b-b7a0-2dce32f94cdc",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Determining the optimal number of clusters in hierarchical clustering involves analyzing the dendrogram and using certain criteria:\n",
    "\n",
    "Dendrogram Cut-off: Cutting the dendrogram at the appropriate level where there is a significant increase in distance between successive merges.\n",
    "Elbow Method: Plotting the total within-cluster variance against the number of clusters and looking for an \"elbow\" point where the rate of decrease sharply slows.\n",
    "Silhouette Analysis: Evaluating the silhouette score for different numbers of clusters and choosing the number that maximizes the average silhouette score.\n",
    "Gap Statistic: Comparing the total within-cluster variance for different numbers of clusters with their expected values under null reference distributions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fee541-0964-4cf2-9d46-92190b3ebe2b",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "Dendrograms are tree-like diagrams that show the arrangement of the clusters produced by hierarchical clustering. They are useful for:\n",
    "\n",
    "Visualizing Cluster Hierarchy: Showing the nested structure of clusters and how they are merged or split at each step.\n",
    "Determining Number of Clusters: Helping to decide the optimal number of clusters by cutting the dendrogram at different levels.\n",
    "Understanding Cluster Relationships: Providing insights into the relative similarity between clusters by the height at which they are merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766c3b2-ac90-4534-9902-2f393d6d8dd9",
   "metadata": {},
   "source": [
    "# Q.6 \n",
    "\n",
    "Yes, hierarchical clustering can be used for both numerical and categorical data, but the distance metrics differ:\n",
    "\n",
    "Numerical Data:\n",
    "\n",
    "Euclidean Distance: Measures the straight-line distance between points in Euclidean space.\n",
    "Manhattan Distance: Measures the sum of absolute differences between coordinates.\n",
    "Minkowski Distance: Generalizes both Euclidean and Manhattan distances.\n",
    "Categorical Data:\n",
    "\n",
    "Hamming Distance: Measures the number of positions at which the corresponding symbols are different.\n",
    "Jaccard Similarity: Measures the similarity between finite sample sets, useful for binary and categorical data.\n",
    "Mixed Data:\n",
    "\n",
    "Gower Distance: A metric that can handle mixed types of data (numerical, categorical, binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bd0bc-1cdc-4f2e-8035-83eb13d0171b",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
