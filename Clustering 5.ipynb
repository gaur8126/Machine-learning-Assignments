{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0902aefd-3753-436a-b4ef-43cfa7dd8392",
   "metadata": {},
   "source": [
    "# Q.1 \n",
    "\n",
    "A contingency matrix, also known as a confusion matrix, is a table used to evaluate the performance of a classification model. It provides a detailed breakdown of how the predicted labels compare to the actual labels.\n",
    "\n",
    "#### Structure:\n",
    "Rows represent the actual classes.\n",
    "Columns represent the predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ec234-c9fc-43b6-b460-d73006376a3d",
   "metadata": {},
   "source": [
    "# Q.2 \n",
    "\n",
    "A pair confusion matrix is used to evaluate clustering algorithms by comparing pairs of points and their cluster assignments. It is different from the regular confusion matrix, which compares individual instances.\n",
    "\n",
    "Structure:\n",
    "\n",
    "Each cell in a pair confusion matrix represents the number of pairs of points that fall into one of four categories:\n",
    "True Positive (TP): Points in the same cluster in both the true labels and predicted labels.\n",
    "True Negative (TN): Points in different clusters in both the true labels and predicted labels.\n",
    "False Positive (FP): Points in different clusters in the true labels but in the same cluster in the predicted labels.\n",
    "False Negative (FN): Points in the same cluster in the true labels but in different clusters in the predicted labels.\n",
    "Usage:\n",
    "\n",
    "It is useful for evaluating the performance of clustering algorithms, especially when ground truth labels are available. Metrics like the Adjusted Rand Index (ARI) can be derived from the pair confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2ea35-fa8d-487e-875f-5dc0b4931e1e",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "An extrinsic measure evaluates a language model based on its performance on an external, real-world task. It measures how well the model contributes to solving a specific application or downstream task.\n",
    "\n",
    "Examples of Extrinsic Measures:\n",
    "\n",
    "Accuracy in text classification tasks (e.g., sentiment analysis).\n",
    "BLEU score in machine translation.\n",
    "F1 score in named entity recognition (NER).\n",
    "Usage:\n",
    "\n",
    "Extrinsic measures assess the practical utility of a language model in specific applications, providing insights into its effectiveness in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d78a8-577f-405b-be11-dbb200a3c398",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "An intrinsic measure evaluates a model based on internal criteria or properties without considering its performance on an external task. It focuses on the quality of the model itself.\n",
    "\n",
    "Examples of Intrinsic Measures:\n",
    "\n",
    "Perplexity for language models.\n",
    "Word error rate for speech recognition models.\n",
    "Coherence score for topic models.\n",
    "Difference from Extrinsic Measures:\n",
    "\n",
    "Intrinsic measures assess the model's inherent quality, while extrinsic measures evaluate its performance in real-world applications. Intrinsic measures are often used during the development and tuning phase to optimize model parameters before applying the model to specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbfbb6-e074-4746-ae04-f63b40801042",
   "metadata": {},
   "source": [
    "# Q.5\n",
    "\n",
    "The purpose of a confusion matrix is to provide a detailed breakdown of the performance of a classification model by comparing the predicted and actual labels.\n",
    "\n",
    "Identifying Strengths and Weaknesses:\n",
    "True Positives (TP): High TP indicates the model is correctly identifying positive instances.\n",
    "True Negatives (TN): High TN indicates the model is correctly identifying negative instances.\n",
    "False Positives (FP): High FP indicates the model is incorrectly labeling negative instances as positive (Type I error).\n",
    "False Negatives (FN): High FN indicates the model is incorrectly labeling positive instances as negative (Type II error).\n",
    "By analyzing the confusion matrix, one can determine:\n",
    "\n",
    "If the model is biased towards a certain class (e.g., predicting more positives than negatives).\n",
    "Which types of errors (FP or FN) are more common and may need addressing.\n",
    "How well the model performs on different classes, helping to identify if it struggles with particular categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5d8bc-007e-4eee-b231-51a3f611bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
