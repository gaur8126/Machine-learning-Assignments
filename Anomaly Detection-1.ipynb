{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80be09f-5110-4490-a261-f0e10b45bd63",
   "metadata": {},
   "source": [
    "# Q.1 \n",
    "\n",
    "### Anomaly Detection \n",
    "Anomaly detection is the process of identifying data points that deviate significantly from the majority of the data, indicating that they may be rare, unusual, or potentially fraudulent. Its purpose is to detect unexpected patterns, errors, or outliers in data that could signify critical issues like fraud, network intrusions, or equipment failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6bbba8-b303-4833-913e-cd9fb47cede5",
   "metadata": {},
   "source": [
    "# Q.2 \n",
    "\n",
    "Key challenges in anomaly detection include:\n",
    "\n",
    "High Dimensionality: Handling and analyzing data with many features.\n",
    "Imbalanced Data: Anomalies are rare compared to normal data points, making it difficult to train models.\n",
    "Dynamic Data: Data distribution can change over time, requiring adaptive algorithms.\n",
    "Noise: Differentiating between true anomalies and noisy data points.\n",
    "Lack of Labeled Data: Often, labeled examples of anomalies are scarce or unavailable, complicating supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f3310-a754-4df9-afff-ca239d8f5796",
   "metadata": {},
   "source": [
    "# Q.3 \n",
    "\n",
    "1. Unsupervised Anomaly Detection: Does not require labeled training data. The algorithm identifies anomalies based on inherent properties of the data, such as density, distance, or clustering. It is useful when labeled examples of anomalies are not available.\n",
    "\n",
    "2. Supervised Anomaly Detection: Requires labeled training data that includes both normal and anomalous examples. The algorithm learns to distinguish between the two classes based on the provided labels, typically achieving higher accuracy when labeled data is abundant and representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f484da6-1384-451f-a144-d2533e940a82",
   "metadata": {},
   "source": [
    "# Q.4 \n",
    "\n",
    "The main categories of anomaly detection algorithms include:\n",
    "\n",
    "Statistical Methods: Use statistical models to detect anomalies based on the probability distribution of the data.\n",
    "Distance-Based Methods: Identify anomalies based on the distance of a data point from its neighbors.\n",
    "Density-Based Methods: Detect anomalies by examining the density of data points in the feature space.\n",
    "Clustering-Based Methods: Use clustering algorithms to find groups of similar data points and identify those that do not belong to any cluster.\n",
    "Machine Learning-Based Methods: Employ supervised or unsupervised learning techniques, including neural networks and ensemble methods like Isolation Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28e94d-393c-4866-92aa-df5bd5e9123b",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "The main assumptions of distance-based anomaly detection methods are:\n",
    "\n",
    "Anomalous points are distant from the majority of data points: They assume that normal data points are closer to each other, forming dense regions, while anomalies are far from these regions.\n",
    "Distance metric relevance: The chosen distance metric (e.g., Euclidean distance) appropriately measures the dissimilarity between data points in the given feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde62ab7-e35a-4173-bed6-6ffe3fdd4c2f",
   "metadata": {},
   "source": [
    "# Q.6 \n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores by comparing the local density of a data point to the local densities of its neighbors. The steps are:\n",
    "\n",
    "k-distance calculation: Compute the k-distance for each data point, which is the distance to its k-th nearest neighbor.\n",
    "Reachability distance: Calculate the reachability distance between each data point and its neighbors.\n",
    "Local reachability density (LRD): Compute the LRD of each data point, which is the inverse of the average reachability distance of the data point from its k-nearest neighbors.\n",
    "LOF score: Calculate the LOF score as the average ratio of the LRD of the data point to the LRDs of its k-nearest neighbors. An LOF score significantly greater than 1 indicates an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331a6f7-7319-43e8-86c0-029accd752b0",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "Number of Trees (n_estimators): The number of trees in the forest, which affects the accuracy and stability of the anomaly score.\n",
    "Subsampling Size (max_samples): The number of samples used to build each tree, affecting the diversity and generalization of the model.\n",
    "Contamination: The proportion of anomalies in the data, used to define the threshold for determining anomalies.\n",
    "Maximum Depth of Trees (max_depth): Limits the depth of the trees, affecting the complexity and computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cf364-948e-4c99-8290-b23dce1a2663",
   "metadata": {},
   "source": [
    "# Q.8 \n",
    "\n",
    "In KNN anomaly detection, the anomaly score is often determined based on the distance to the k-th nearest neighbor. Given that the data point has only 2 neighbors within a radius of 0.5, which is much less than K=10, the point is likely to be considered an anomaly. The specific anomaly score will depend on the implementation details, but the data point's limited number of neighbors compared to the required K will result in a high anomaly score, indicating it is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e67f06-d68f-486d-8e59-550db7f6c802",
   "metadata": {},
   "source": [
    "# Q.9\n",
    "\n",
    "Anamoly Score = 2^-E(h(x))/c(m)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
