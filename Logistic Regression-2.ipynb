{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28296b7-85b8-48a2-a965-29c49ab968d9",
   "metadata": {},
   "source": [
    "## Q.1 \n",
    "\n",
    "Purpose:\n",
    "\n",
    "Grid Search Cross-Validation (Grid Search CV) is a technique used to find the optimal hyperparameters for a machine learning model. The goal is to enhance model performance by systematically evaluating combinations of hyperparameters and selecting the best set based on cross-validation performance.\n",
    "\n",
    "How it Works:\n",
    "\n",
    "Define Hyperparameter Grid: Specify a grid of hyperparameters to be evaluated. For example, for a logistic regression model, this might include different values of the regularization parameter (C) and different types of regularization (L1, L2).\n",
    "\n",
    "Cross-Validation: For each combination of hyperparameters, perform k-fold cross-validation to evaluate the model’s performance.\n",
    "Evaluate Performance: Calculate the average performance metric (e.g., accuracy, F1-score) for each combination across the folds.\n",
    "\n",
    "Select Best Hyperparameters: Identify the combination of hyperparameters that results in the best cross-validation performance.\n",
    "\n",
    "Train Final Model: Train the final model on the entire training dataset using the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fcc27-7552-4f03-8ffa-ed911c93c763",
   "metadata": {},
   "source": [
    "## Q.2 \n",
    "\n",
    "Grid Search CV:\n",
    "\n",
    "Exhaustive Search: Evaluates all possible combinations of the hyperparameters specified in the grid.\n",
    "Time-Consuming: Can be computationally expensive and time-consuming, especially with large grids and datasets.\n",
    "Randomized Search CV:\n",
    "\n",
    "Random Sampling: Randomly samples a specified number of hyperparameter combinations from the grid.\n",
    "Faster: Generally faster than grid search because it does not evaluate every possible combination.\n",
    "When to Choose:\n",
    "\n",
    "Grid Search CV: Use when the hyperparameter space is small and computational resources allow for an exhaustive search.\n",
    "Randomized Search CV: Use when the hyperparameter space is large or when computational resources and time are limited. It allows for a broader search with fewer evaluations, which is beneficial for complex models and large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469e4a7-714e-4564-a2dc-1e0f07391ab4",
   "metadata": {},
   "source": [
    "## Q.3 \n",
    "\n",
    "Data Leakage:\n",
    "Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates and poor generalization to new data.\n",
    "\n",
    "Problem:\n",
    "It creates an unfair advantage during training, causing the model to perform well on training and validation data but poorly on unseen data. This undermines the model’s predictive validity.\n",
    "\n",
    "Example:\n",
    "Suppose you are building a model to predict whether a customer will default on a loan. If the dataset includes a feature that is a proxy for the target variable, such as a “default indicator” flag that was set after the loan was issued, this would cause data leakage. The model would learn to predict based on this feature, which would not be available in real-world scenarios when making predictions for new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c06602-f6e9-4c6c-b9b1-84e98f938481",
   "metadata": {},
   "source": [
    "## Q.4 \n",
    "\n",
    "Preventing Data Leakage:\n",
    "\n",
    "Proper Data Splitting: Ensure that training, validation, and test sets are properly separated and that no information from the test set leaks into the training process.\n",
    "Feature Engineering: Perform feature engineering separately for training and validation/test data to prevent information about the test set from influencing the model.\n",
    "Pipeline Usage: Use data preprocessing pipelines to ensure that transformations are applied consistently and only on the training data during model fitting.\n",
    "Cross-Validation: During cross-validation, ensure that the same preprocessing steps are applied within each fold without information leakage between folds.\n",
    "Temporal Data Handling: For time-series data, ensure that future information is not used to predict past events by splitting data chronologically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2d071-b39c-4c73-b65d-ca7d32e0013c",
   "metadata": {},
   "source": [
    "## Q.5 \n",
    "\n",
    "Confusion Matrix:\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing the predicted labels against the actual labels. It provides a detailed breakdown of correct and incorrect classifications.\n",
    "\n",
    "Structure:\n",
    "\n",
    "1. True Positives (TP): Correctly predicted positive instances.\n",
    "2. True Negatives (TN): Correctly predicted negative instances.\n",
    "3. False Positives (FP): Incorrectly predicted positive instances (Type I error).\n",
    "4. False Negatives (FN): Incorrectly predicted negative instances (Type II error).\n",
    "\n",
    "Performance Insights:\n",
    "The confusion matrix helps in understanding:\n",
    "\n",
    "1. The number of correct and incorrect predictions.\n",
    "2. Types of errors the model makes.\n",
    "3. Balance between different classes' predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b1b0c-88b6-4ddc-a7ae-ae176cbcb5fd",
   "metadata": {},
   "source": [
    "##  Q.6 \n",
    "\n",
    "Difference between precision and recall \n",
    "\n",
    "## Precision\n",
    "formula- TP/(TP+FP) [out of all the actual values how many are correctly predict]\n",
    "* most important is false positive \n",
    "\n",
    "## Recall \n",
    "formula - TP/(TP+FN) [out of all the predicted values how  many are correctly predict with actual values  ]\n",
    "* most important is false negative \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942d929-82ae-41db-bcbe-162e0766b366",
   "metadata": {},
   "source": [
    "## Q.7 \n",
    "False Positives (FP): Indicates instances where the model incorrectly predicted the positive class. High FP suggests the model is too lenient in predicting positives.\n",
    "\n",
    "False Negatives (FN): Indicates instances where the model incorrectly predicted the negative class. High FN suggests the model is missing actual positives.\n",
    "\n",
    "Balance of Errors: By comparing FP and FN, you can assess whether the model is biased towards predicting one class over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d39ee-1c2a-4f00-9490-4d97c6adf64c",
   "metadata": {},
   "source": [
    "## Q.8\n",
    "\n",
    "Commen metrix- \n",
    "\n",
    "1. Accuracy - (TP+TN)/(TP+TN+FP+FN)\n",
    "2. Precision - TP/(TP+FP)\n",
    "3. Recall - TP/(TP+FN)\n",
    "4. F1 score - 2 x (precision x recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f54c4c-1395-47a7-a105-3769c3bc5453",
   "metadata": {},
   "source": [
    "## Q.9 \n",
    "\n",
    "Accuracy is derived from the confusion matrix as the ratio of correct predictions (sum of TP and TN) to the total number of predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880422b-698d-4f78-a3db-50a4078c891f",
   "metadata": {},
   "source": [
    "## Q.10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
