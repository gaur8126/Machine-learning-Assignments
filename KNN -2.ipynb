{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811bca26-11eb-4d20-97a3-0045afbebb3c",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "##### Main Difference:\n",
    "\n",
    "* Euclidean Distance: Measures the straight-line distance between two points in Euclidean space. It is sensitive to differences in all dimensions.\n",
    "* Manhattan Distance: Measures the distance between two points along the axes at right angles. It is the sum of the absolute differences of their coordinates.\n",
    "\n",
    "#### Effect on Performance:\n",
    "\n",
    "* Euclidean Distance: Tends to be more sensitive to large differences in any single dimension, which can dominate the distance calculation. It is suitable for data with fewer dimensions and where all features contribute equally.\n",
    "* Manhattan Distance: Less sensitive to outliers and large differences in individual dimensions. It is often better for high-dimensional data or when the differences in individual dimensions are more meaningful.\n",
    "\n",
    "##### Performance Implications:\n",
    "\n",
    "* KNN Classifier: The choice of distance metric can affect the boundaries between classes. Euclidean distance might lead to smoother boundaries, while Manhattan distance can create more orthogonal boundaries.\n",
    "* KNN Regressor: The predicted values might vary more smoothly with Euclidean distance, while Manhattan distance can lead to more block-like variations in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48388d-ef69-403b-a2fa-9e7e89754d33",
   "metadata": {},
   "source": [
    "# Q.2\n",
    "\n",
    "### The techniques to choose the optimal value of K\n",
    "\n",
    "1. Cross Velidation\n",
    "2. Elbow Method\n",
    "3. Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc5db6-7b9f-457d-b678-0ae791446238",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "### Effect on Performance:\n",
    "\n",
    "The choice of distance metric influences how distances are calculated between points, which in turn affects the neighbors selected for each point.\n",
    "1. Euclidean Distance: More suitable for data where the features are continuous and where the distance in all dimensions is equally important.\n",
    "2. Manhattan Distance: Better for data with high dimensions or when features are on different scales or less sensitive to outliers.\n",
    "\n",
    "### Situations to Choose:\n",
    "\n",
    "1. Euclidean Distance: Use when the data is normalized, and the features are of similar scale and contribute equally to the outcome.\n",
    "2. Manhattan Distance: Use when dealing with high-dimensional data, when features have different scales, or when robustness to outliers is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4e157-ee14-46f2-88f1-9734c543e817",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Common Hyperparameters:\n",
    "\n",
    "Number of Neighbors (k): Determines the number of nearest neighbors to consider. A smaller k can lead to a model that is too sensitive (high variance), while a larger k can lead to oversmoothing (high bias).\n",
    "\n",
    "Distance Metric: Choices include Euclidean, Manhattan, Minkowski, etc. The choice of metric affects how distances are calculated and which points are considered neighbors.\n",
    "\n",
    "Weights: Determines whether all neighbors are weighted equally or if closer neighbors have more influence (e.g., uniform vs. distance weighting).\n",
    "\n",
    "Tuning Hyperparameters:\n",
    "\n",
    "Grid Search: Systematically explore combinations of hyperparameters using cross-validation to find the optimal settings.\n",
    "Random Search: Randomly sample hyperparameter combinations and evaluate performance, which can be more efficient for large hyperparameter spaces.\n",
    "Bayesian Optimization: Use probabilistic models to guide the search for optimal hyperparameters based on past evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b68bd2-daf7-46c6-8a1d-0e2c59f4dc55",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "### Effect of Training Set Size:\n",
    "\n",
    "Larger Training Set: Generally improves the performance of KNN because it provides more data points to determine the nearest neighbors, leading to better generalization.\n",
    "Smaller Training Set: May lead to overfitting and poorer generalization because the model has fewer data points to rely on.\n",
    "\n",
    "### Techniques to Optimize Training Set Size:\n",
    "\n",
    "Data Augmentation: Generate additional data points through techniques like rotation, scaling, or noise addition to increase the size of the training set.\n",
    "\n",
    "Feature Selection: Reduce the dimensionality of the data by selecting the most relevant features, which can make the algorithm more efficient and potentially improve performance.\n",
    "\n",
    "Sampling Techniques: Use stratified sampling to ensure that the training set is representative of the entire dataset.\n",
    "\n",
    "Cross-Validation: Use cross-validation to make the most of the available data by training and validating the model on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ac604-76c8-4caf-86f6-88f82b627e2b",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "\n",
    "### Potential Drawbacks:\n",
    "\n",
    "Computational Complexity: KNN is computationally expensive, especially for large datasets, because it requires calculating the distance to every training point during prediction.\n",
    "Curse of Dimensionality: Performance degrades with high-dimensional data because distances become less meaningful.\n",
    "Sensitivity to Irrelevant Features: Irrelevant or redundant features can distort distance calculations and degrade performance.\n",
    "Imbalanced Data: KNN can perform poorly if the dataset is imbalanced, as the majority class can dominate the predictions.\n",
    "\n",
    "### Overcoming Drawbacks:\n",
    "\n",
    "Efficient Data Structures: Use data structures like KD-trees or Ball-trees to speed up the nearest neighbor search.\n",
    "\n",
    "Dimensionality Reduction: Apply techniques like Principal Component Analysis (PCA) or t-SNE to reduce the number of dimensions.\n",
    "\n",
    "Feature Selection: Use feature selection methods to remove irrelevant or redundant features.\n",
    "\n",
    "Normalization: Normalize or standardize the features to ensure they contribute equally to distance calculations.\n",
    "\n",
    "Handling Imbalanced Data: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset or modify the distance metric to account for class imbalance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
