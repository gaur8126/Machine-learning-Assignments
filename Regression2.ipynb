{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c3a913-e1a1-4f50-8990-c675a5439996",
   "metadata": {},
   "source": [
    "## Q.1\n",
    "\n",
    "R-squared in Linear Regression Models : \n",
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables in a regression model.\n",
    "\n",
    "It measures the goodness of fit of the regression model.\n",
    "Higher R-squared values indicate a better fit of the model to the data.\n",
    "\n",
    "calculatio : \n",
    "\n",
    "R-squared = 1 - SSR/SST\n",
    "where, \n",
    "SSR = sum of Square residial(error)\n",
    "SST  = sum of square total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d213b-c129-482c-92b1-ba4585a47281",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "\n",
    "Adjusted R-squared\n",
    "\n",
    "Adjusted R-squared is a modified version of the R-squared statistic that adjusts for the number of predictors in the model. It provides a more accurate measure of the goodness of fit, especially when multiple predictors are involved.\n",
    "\n",
    "\n",
    "R-squared: Adding more predictors to the model can never decrease the R-squared value. This can lead to overfitting, where the model appears to have a better fit because it captures random noise in the data.\n",
    "\n",
    "Adjusted R-squared: It can decrease if the added predictors do not improve the model. This helps to mitigate the risk of overfitting by penalizing the addition of unnecessary predictors.\n",
    "\n",
    "R-squared > adjusted R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579c872-c460-4388-ab6e-45ac79ba5771",
   "metadata": {},
   "source": [
    "## Q.3\n",
    "\n",
    "Adjusted R-squared is more appropiate for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdbdfa-9d86-4831-a875-56f15dd139a1",
   "metadata": {},
   "source": [
    "## Q.4\n",
    "\n",
    "\n",
    "RMSE, MSE, and MAE in Regression Analysis\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common metrics used to evaluate the performance of regression models. These metrics provide insights into the accuracy of the predictions made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb1d43-807c-4f0e-8a18-68d3c4a47644",
   "metadata": {},
   "source": [
    "## Q.5\n",
    "\n",
    "Advantages and disadvantages of MSE,RMSE,MAE\n",
    "\n",
    "Advantages - \n",
    "MSE- 1. Equation is differenciable , 2. it has only one local and gloabl minima \n",
    "RMSE - 1.Equation is differenciable , 2. Always in same unit\n",
    "MAE - 1.it is useful to robust  to outliers, 2. It will be in the same unit \n",
    "\n",
    "Disadvantages - \n",
    "MAE - 1.Not robust to outliers , 2. It is not  in the same unit \n",
    "RMSE- 1.  Not robust to outliers\n",
    "MAE - 1. convergence usually take more time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b47dd9-82ff-4f6b-a352-0f77b651721d",
   "metadata": {},
   "source": [
    "## Q.6 \n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in regression analysis to enhance the prediction accuracy and interpretability of the model by adding a penalty term to the loss function. This penalty term is proportional to the sum of the absolute values of the coefficients.\n",
    "\n",
    "Lasso regression usually used for Feature selection and on the other hand Ridge regression is used for reducing overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9d6b1-bd2f-40aa-b0ea-9c03214f70ab",
   "metadata": {},
   "source": [
    "## Q.7 \n",
    "\n",
    "How Regularized Linear Models Help Prevent Overfitting\n",
    "Regularized linear models, such as Lasso and Ridge regression, help prevent overfitting by adding a penalty term to the loss function. This penalty discourages the model from fitting the noise in the training data too closely, which can lead to better generalization on new, unseen data.\n",
    "\n",
    "When our model has high testing error is called overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd4878d-5566-4350-af7b-a65432da33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Training MSE: 0.2185086272725248\n",
      "Linear Regression - Test MSE: 0.2484655963354167\n",
      "Ridge Regression - Training MSE: 0.22249463723800686\n",
      "Ridge Regression - Test MSE: 0.25866081223030235\n",
      "Lasso Regression - Training MSE: 0.4962445180930407\n",
      "Lasso Regression - Test MSE: 0.8842904250904079\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 20)\n",
    "true_coefs = np.random.randn(20)\n",
    "y = X @ true_coefs + np.random.randn(100) * 0.5  # Adding noise\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit a linear regression model (without regularization)\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "print(\"Linear Regression - Training MSE:\", mean_squared_error(y_train, y_pred_train))\n",
    "print(\"Linear Regression - Test MSE:\", mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# Fit a Ridge regression model (with L2 regularization)\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "y_pred_train_ridge = ridge_reg.predict(X_train)\n",
    "y_pred_test_ridge = ridge_reg.predict(X_test)\n",
    "print(\"Ridge Regression - Training MSE:\", mean_squared_error(y_train, y_pred_train_ridge))\n",
    "print(\"Ridge Regression - Test MSE:\", mean_squared_error(y_test, y_pred_test_ridge))\n",
    "\n",
    "# Fit a Lasso regression model (with L1 regularization)\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "y_pred_train_lasso = lasso_reg.predict(X_train)\n",
    "y_pred_test_lasso = lasso_reg.predict(X_test)\n",
    "print(\"Lasso Regression - Training MSE:\", mean_squared_error(y_train, y_pred_train_lasso))\n",
    "print(\"Lasso Regression - Test MSE:\", mean_squared_error(y_test, y_pred_test_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9834cb4-7775-4af1-894e-82e627389d86",
   "metadata": {},
   "source": [
    "## Q.8 \n",
    "\n",
    "Limitations of Regularized Linear Models\n",
    "While regularized linear models such as Ridge and Lasso regression are powerful tools for mitigating overfitting and improving model generalization, they have certain limitations and may not always be the best choice for regression analysis. Here are some of the key limitations:\n",
    "\n",
    "Assumption of Linearity:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the predictors and the response variable. If the true relationship is nonlinear, these models may not perform well, even with regularization.\n",
    "Nonlinear patterns in the data can be better captured by other methods like polynomial regression, decision trees, or neural networks.\n",
    "Sensitivity to the Choice of Regularization Parameter:\n",
    "\n",
    "The performance of regularized models heavily depends on the choice of the regularization parameter (\n",
    "ùúÜ\n",
    "Œª for Ridge and Lasso). Selecting the optimal value typically requires cross-validation, which can be computationally intensive.\n",
    "Incorrect choice of \n",
    "ùúÜ\n",
    "Œª can lead to either underfitting (too much regularization) or overfitting (too little regularization).\n",
    "Computational Complexity:\n",
    "\n",
    "For very large datasets with high dimensionality, regularized models can become computationally expensive, particularly Lasso regression, which involves solving a complex optimization problem.\n",
    "While algorithms like coordinate descent have made Lasso more tractable, the computational burden can still be significant for very large-scale problems.\n",
    "Handling Multicollinearity:\n",
    "\n",
    "While Ridge regression can mitigate the effects of multicollinearity by shrinking coefficients, it does not eliminate the problem entirely.\n",
    "Lasso regression can perform variable selection and might drop some correlated predictors, but this can lead to instability in the model when predictors are highly correlated.\n",
    "Feature Selection in Lasso:\n",
    "\n",
    "Lasso‚Äôs feature selection can be both an advantage and a limitation. If the true model is not sparse (i.e., many predictors are relevant), Lasso might drop important variables, leading to underfitting.\n",
    "Lasso can also be unstable when the number of predictors is much larger than the number of observations, as small changes in the data can lead to large changes in the selected model.\n",
    "Interpretability:\n",
    "\n",
    "While regularized models can simplify interpretation by reducing the number of predictors (in the case of Lasso), the resulting coefficients can still be difficult to interpret, especially when interactions or nonlinearities are present.\n",
    "Other techniques like decision trees or rule-based models might provide more intuitive interpretations.\n",
    "When Regularized Linear Models May Not Be the Best Choice\n",
    "Nonlinear Relationships:\n",
    "\n",
    "If the underlying relationship between predictors and the response variable is nonlinear, techniques such as polynomial regression, support vector machines, decision trees, or neural networks may be more appropriate.\n",
    "Complex Interactions:\n",
    "\n",
    "When there are complex interactions between predictors, methods that can model interactions explicitly, such as decision trees, random forests, or gradient boosting machines, might be more suitable.\n",
    "High-Dimensional, Sparse Data:\n",
    "\n",
    "In high-dimensional, sparse datasets (e.g., text data, genomic data), methods like support vector machines with appropriate kernels or specialized techniques like L1-regularized logistic regression for classification might be more effective.\n",
    "Computational Constraints:\n",
    "\n",
    "When computational resources are limited, simpler models or models with faster training times, like decision trees or certain ensemble methods, might be preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dc59e-bedb-4aab-b3ac-d002a4842c99",
   "metadata": {},
   "source": [
    "## Q.9\n",
    "\n",
    "Sensitivity to Outliers:\n",
    "\n",
    "Model A (RMSE): If your problem is sensitive to larger errors and you want to penalize them more heavily, RMSE is more appropriate.\n",
    "Model B (MAE): If you prefer a metric that provides a more balanced view of the average error and is less affected by outliers, MAE is more appropriate.\n",
    "Nature of the Data:\n",
    "\n",
    "If the data contains outliers or a few large errors, RMSE will reflect this more than MAE.\n",
    "If the errors are more uniformly distributed, MAE might give a clearer picture of model performance.\n",
    "Consistency:\n",
    "\n",
    "Since the two models are evaluated using different metrics, a direct comparison is challenging. Ideally, both models should be evaluated using the same metric for a fair comparison.\n",
    "Conclusion\n",
    "Given the metrics provided:\n",
    "\n",
    "Model B with MAE of 8 appears to have a lower average error compared to Model A with RMSE of 10. However, this comparison is not entirely fair without knowing the MAE for Model A and the RMSE for Model B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4661f7-3ae1-4a8d-abc1-fed826481397",
   "metadata": {},
   "source": [
    "## Q.10\n",
    "\n",
    "Factors to Consider in Choosing the Better Model\n",
    "Model Performance (Validation Metrics):\n",
    "\n",
    "The ultimate decision should be based on model performance metrics (e.g., RMSE, MAE) on a validation set. These metrics provide a direct comparison of how well each model generalizes to unseen data.\n",
    "Nature of the Data and Problem:\n",
    "\n",
    "High Dimensionality and Feature Selection: If the dataset has many features, some of which are potentially irrelevant or redundant, Lasso (Model B) might be preferable due to its ability to perform feature selection by setting some coefficients to zero.\n",
    "Multicollinearity: If the features are highly correlated, Ridge (Model A) might be better because it tends to handle multicollinearity by distributing the coefficients among the correlated variables.\n",
    "Regularization Parameter (\n",
    "ùúÜ\n",
    "Œª):\n",
    "\n",
    "The choice of \n",
    "ùúÜ\n",
    "Œª affects the degree of regularization. Comparing a Ridge model with \n",
    "ùúÜ\n",
    "=\n",
    "0.1\n",
    "Œª=0.1 and a Lasso model with \n",
    "ùúÜ\n",
    "=\n",
    "0.5\n",
    "Œª=0.5 might not be entirely fair, as they represent different strengths of regularization. Ideally, one should perform cross-validation to find the optimal \n",
    "ùúÜ\n",
    "Œª for each model.\n",
    "Interpretability:\n",
    "\n",
    "If model interpretability is important and you need a sparse model where some coefficients are exactly zero, Lasso (Model B) is more appropriate.\n",
    "If you are more concerned with minimizing overall prediction error and are less worried about the number of non-zero coefficients, Ridge (Model A) might be better.\n",
    "Trade-offs and Limitations\n",
    "Bias-Variance Trade-off:\n",
    "\n",
    "Ridge tends to have lower variance but might introduce more bias compared to Lasso.\n",
    "Lasso might introduce more variance, especially if it sets many coefficients to zero, but it can reduce bias by excluding irrelevant features.\n",
    "Model Complexity:\n",
    "\n",
    "Ridge maintains all features but shrinks their coefficients, leading to less drastic model simplification.\n",
    "Lasso can lead to simpler models by eliminating some features, but this can also mean missing out on important features if \n",
    "ùúÜ\n",
    "Œª is not tuned properly.\n",
    "Computational Cost:\n",
    "\n",
    "Lasso can be computationally more intensive, especially for very high-dimensional data, due to the nature of the optimization problem.\n",
    "Conclusion and Recommendation\n",
    "To choose the better performer between Model A (Ridge) and Model B (Lasso), you should:\n",
    "\n",
    "Evaluate Both Models on the Same Metric: Compare the performance metrics (e.g., RMSE, MAE) of both models on a validation set. This direct comparison is crucial for making an informed decision.\n",
    "Consider Feature Selection Needs: If reducing the number of features is important for interpretability or reducing model complexity, Lasso might be the better choice.\n",
    "Analyze Regularization Parameters: Ensure that the regularization parameters are optimally chosen for both models using cross-validation.\n",
    "Practical Approach\n",
    "Cross-validation: Perform cross-validation to find the optimal \n",
    "ùúÜ\n",
    "Œª for both Ridge and Lasso models.\n",
    "Model Comparison: Compare the models using the same evaluation metrics on a validation set.\n",
    "Contextual Considerations: Consider the nature of the data and the importance of feature selection or handling multicollinearity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
