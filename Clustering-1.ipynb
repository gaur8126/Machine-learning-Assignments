{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4cfc24-8878-4240-bbe9-0c203de2fa6d",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "Partitioning Methods:\n",
    "\n",
    "K-means: Divides the data into K clusters by minimizing the sum of squared distances between points and their centroids. Assumes clusters are spherical and equally sized.\n",
    "K-medoids (PAM): Similar to K-means but uses actual data points (medoids) as cluster centers, making it more robust to noise and outliers.\n",
    "Hierarchical Methods:\n",
    "\n",
    "Agglomerative: Starts with each point as its own cluster and iteratively merges the closest pairs of clusters. Assumes a nested clustering structure.\n",
    "Divisive: Starts with one cluster containing all points and recursively splits it into smaller clusters.\n",
    "Density-Based Methods:\n",
    "\n",
    "DBSCAN: Groups points that are closely packed together while marking points in low-density regions as outliers. Assumes clusters of arbitrary shape and varying densities.\n",
    "OPTICS: Similar to DBSCAN but handles varying densities better by ordering points to reflect their density-based clustering structure.\n",
    "Grid-Based Methods:\n",
    "\n",
    "STING: Divides the data space into a grid structure and performs clustering on the grid cells. Assumes a uniform data distribution within each grid cell.\n",
    "Model-Based Methods:\n",
    "\n",
    "Gaussian Mixture Models (GMM): Assumes data is generated from a mixture of several Gaussian distributions and uses the Expectation-Maximization algorithm to estimate the parameters.\n",
    "Bayesian Clustering: Uses Bayesian inference to determine the number of clusters and their parameters.\n",
    "Spectral Clustering:\n",
    "\n",
    "Uses the eigenvalues of a similarity matrix to perform dimensionality reduction before applying clustering in the reduced space. Assumes clusters are more distinguishable in a transformed space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b52e86-141c-442e-8635-46264c9deadf",
   "metadata": {},
   "source": [
    "# Q.2 \n",
    "K-means clustering is a partitioning method that aims to divide a set of n observations into K clusters, where each observation belongs to the cluster with the nearest mean. The algorithm works as follows:\n",
    "\n",
    "Initialization: Choose K initial centroids randomly.\n",
    "Assignment Step: Assign each data point to the nearest centroid based on Euclidean distance.\n",
    "Update Step: Recalculate the centroids as the mean of all points assigned to each centroid.\n",
    "Iteration: Repeat the assignment and update steps until the centroids no longer change or a maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c702a82-d4fc-4c01-989c-395ac46f2bef",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simplicity: Easy to implement and understand.\n",
    "Scalability: Efficient for large datasets.\n",
    "Speed: Typically faster than hierarchical and density-based methods.\n",
    "Limitations:\n",
    "\n",
    "Fixed number of clusters: Requires specifying the number of clusters (K) in advance.\n",
    "Sensitivity to initialization: Results can vary depending on the initial choice of centroids.\n",
    "Assumes spherical clusters: Works best when clusters are well-separated and equally sized.\n",
    "Not robust to outliers: Outliers can significantly affect the placement of centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c76c4-91e0-438e-a6cd-0871b407e08f",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Elbow Method: Plot the sum of squared distances from each point to its assigned centroid (within-cluster sum of squares) for different values of K. The optimal K is often found at the \"elbow\" point where the rate of decrease sharply slows.\n",
    "Silhouette Score: Measures how similar a point is to its own cluster compared to other clusters. Values range from -1 to 1, with higher values indicating better-defined clusters. The optimal K maximizes the average silhouette score.\n",
    "Gap Statistic: Compares the total within-cluster variation for different values of K with their expected values under null reference distributions of the data.\n",
    "Cross-Validation: Splits the data into training and validation sets, runs K-means on the training set, and evaluates the clustering quality on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82dadf-8cee-4606-ad6a-24ebd31e9131",
   "metadata": {},
   "source": [
    "# Q.5\n",
    "\n",
    "Market Segmentation: Grouping customers based on purchasing behavior, demographics, or other characteristics to target marketing strategies more effectively.\n",
    "Image Compression: Reducing the number of colors in an image by clustering pixel values and replacing them with the cluster centroids.\n",
    "Anomaly Detection: Identifying unusual patterns in data, such as fraud detection in financial transactions by clustering normal transaction patterns.\n",
    "Document Clustering: Organizing a large collection of documents into topics or categories for easier retrieval and analysis.\n",
    "Genomic Data Analysis: Grouping similar gene expression profiles to identify distinct biological states or conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcdf6f-123d-47c2-8553-fb7e4d4dcf91",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "\n",
    "Cluster Centroids: The coordinates of the centroids represent the average characteristics of the points in each cluster. Analyzing these can reveal common traits or patterns within each group.\n",
    "Cluster Sizes: The number of points in each cluster can indicate the prevalence or importance of certain patterns or categories in the data.\n",
    "Intra-Cluster Distance: The sum of squared distances within each cluster can provide insights into the compactness and coherence of clusters.\n",
    "Inter-Cluster Distance: The distances between centroids can help understand the separation and distinctness of different clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830803b-6131-4fab-b283-90ad2d4405d7",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
