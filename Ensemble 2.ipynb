{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc2f606-5353-4cab-a2c6-59195396c5f1",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by averaging multiple decision trees trained on different bootstrap samples of the training data. Hereâ€™s how it works:\n",
    "\n",
    "Bootstrap Sampling: Multiple subsets of the training data are created by random sampling with replacement.\n",
    "Training: A decision tree is trained on each bootstrap sample. Each tree may overfit its respective sample, but the overfitting patterns will be different for each tree.\n",
    "Aggregation: The predictions from all trees are averaged (in regression) or a majority vote is taken (in classification) to produce the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3ff82-db60-4979-b4bf-15ecd6ed234b",
   "metadata": {},
   "source": [
    "# Q.2\n",
    "\n",
    "Advantages:\n",
    "Flexibility: Different base learners can be adapted to suit the specific problem. For example, simpler models might be more interpretable, while more complex models might capture more intricate patterns.\n",
    "Reduced Bias: Using stronger base learners (like deeper trees or more complex models) can reduce bias if the base learner is inherently more capable of capturing the underlying data patterns.\n",
    "Diverse Perspectives: Different types of base learners can offer diverse perspectives on the data, potentially leading to a more comprehensive model.\n",
    "Disadvantages:\n",
    "Complexity and Computation: More complex base learners require more computational resources, both for training and prediction.\n",
    "Risk of Overfitting: While bagging generally reduces overfitting, using overly complex base learners can still lead to models that overfit the data.\n",
    "Hyperparameter Tuning: Different base learners come with their own set of hyperparameters, which can increase the complexity of model tuning and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0318c57-fbd1-4f78-88ba-be2d09dd7c0f",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "Simple Base Learners (e.g., shallow decision trees):\n",
    "\n",
    "Higher Bias, Lower Variance: Simple models tend to underfit the data, resulting in higher bias but lower variance.\n",
    "Effect in Bagging: Bagging can reduce variance effectively, but the high bias might still be a limiting factor for the overall model performance.\n",
    "Complex Base Learners (e.g., deep decision trees):\n",
    "\n",
    "Lower Bias, Higher Variance: Complex models can capture more details of the data, resulting in lower bias but higher variance.\n",
    "Effect in Bagging: Bagging is particularly effective in this case because it can significantly reduce the high variance of complex models, leading to a lower overall error.\n",
    "Thus, the optimal choice of base learner balances the inherent bias-variance characteristics with the effectiveness of bagging in reducing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154c2cc-0b39-440c-b8e8-70f9e272d52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
