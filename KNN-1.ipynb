{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f574f7-f73f-47b4-9cda-456e88e26a5d",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression. In KNN, the input consists of the k closest training examples in the feature space. The output depends on whether KNN is used for classification or \n",
    "\n",
    "regression:\n",
    "\n",
    "Classification: The output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. \n",
    "\n",
    "Regression: The output is the average of the values of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882caac-2c16-462e-960a-59c02bfcf8c2",
   "metadata": {},
   "source": [
    "# Q.2\n",
    "\n",
    "Choosing the value of k is crucial for the performance of the KNN algorithm. Here are some common methods to determine the optimal k:\n",
    "\n",
    "Cross-Validation: Use cross-validation techniques to select the k value that yields the best performance on the validation set.\n",
    "Elbow Method: Plot the error rate against various values of k and choose the k where the error rate starts to diminish at a slower rate (the \"elbow point\").\n",
    "Domain Knowledge: Sometimes, domain-specific knowledge can help in selecting an appropriate k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f4223-9c35-4c3f-8c12-f15c9044a981",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "The main difference between KNN classifier and KNN regressor lies in their output and the way they handle the nearest neighbors:\n",
    "\n",
    "KNN Classifier: Predicts the class label by majority voting among the k nearest neighbors.\n",
    "KNN Regressor: Predicts a continuous value as the average (or weighted average) of the k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c0cee-86d8-4f3f-b414-de3ee2e0a9d2",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Performance of KNN can be measured using different metrics depending on whether it is used for classification or regression:\n",
    "\n",
    "Classification Metrics:\n",
    "\n",
    "* Accuracy\n",
    "* Precision, Recall, and F1-Score\n",
    "* Confusion Matrix\n",
    "* ROC Curve and AUC\n",
    "\n",
    "Regression Metrics:\n",
    "\n",
    "* Mean Absolute Error (MAE)\n",
    "* Mean Squared Error (MSE)\n",
    "* Root Mean Squared Error (RMSE)\n",
    "* ùëÖ2 Score (Coefficient of Determination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c21765-b4f5-4c79-98af-7d75526c540c",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces. For KNN, as the number of dimensions increases, the distance between points becomes less meaningful, and all points tend to become equidistant from each other. This leads to reduced performance of the KNN algorithm because it becomes harder to find the nearest neighbors accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb442e4-51d3-4d08-bd3d-813c699503a4",
   "metadata": {},
   "source": [
    "# Q.6 \n",
    "\n",
    "1. Imputation: Replace missing values with the mean, median, or mode of the feature.\n",
    "2. Deletion: Remove instances with missing values (if they are few and not representative).\n",
    "3. KNN Imputation: Use a KNN algorithm to impute missing values by finding the nearest neighbors and averaging their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f69c15-d451-4dd0-a93b-c97ff5b17cc2",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "### KNN Classifier:\n",
    "\n",
    "* Strengths: Simple, intuitive, effective for smaller datasets and lower dimensions, non-parametric.\n",
    "* Weaknesses: Sensitive to the choice of k, computationally expensive for large datasets, affected by irrelevant or redundant features.\n",
    "* Best for: Classification tasks with well-separated classes and lower-dimensional data.\n",
    "\n",
    "### KNN Regressor:\n",
    "\n",
    "* Strengths: Simple, intuitive, effective for predicting continuous outcomes, non-parametric.\n",
    "* Weaknesses: Sensitive to the choice of k, computationally expensive for large datasets, affected by noise and irrelevant features.\n",
    "* Best for: Regression tasks where the relationship between the variables is non-linear and the dataset is not too large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8bd1e-dbb8-4e99-94c6-669f947cef0f",
   "metadata": {},
   "source": [
    "# Q.8 \n",
    "\n",
    "Strengths:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Non-parametric and flexible.\n",
    "No training phase (lazy learner).\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive during prediction.\n",
    "Sensitive to the choice of k.\n",
    "Performance degrades with high-dimensional data (curse of dimensionality).\n",
    "Sensitive to irrelevant and redundant features.\n",
    "\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Use feature selection or dimensionality reduction techniques like PCA.\n",
    "Normalize or standardize the data.\n",
    "Use efficient algorithms for nearest neighbor search like KD-trees or Ball-trees.\n",
    "Optimize k using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb00615-941f-4e34-8b04-7682ccd7cacb",
   "metadata": {},
   "source": [
    "# Q.9\n",
    "\n",
    "Euclidean Distance: The straight-line distance between two points in Euclidean space. It is calculated as:\n",
    "\n",
    "* d = ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "\n",
    "Manhattan Distance: The distance between two points measured along the axes at right angles (also known as L1 distance). It is calculated as:\n",
    "\n",
    "* d = |x2-x1| + |y2-y1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e903f6e-808d-4a8a-8df0-72f9677044b0",
   "metadata": {},
   "source": [
    "# Q.10\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
