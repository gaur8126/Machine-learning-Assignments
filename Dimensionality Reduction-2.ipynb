{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfee9a6-bcd6-4ef2-ac0a-ae51ba6e1a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q.1\n",
    "\n",
    "### Projection:\n",
    "\n",
    "In the context of PCA (Principal Component Analysis), projection refers to the process of transforming data points from their original high-dimensional space onto a new subspace defined by the principal components.\n",
    "Each principal component is a direction (a vector) in the feature space along which the data has the maximum variance.\n",
    "\n",
    "### Usage in PCA:\n",
    "\n",
    "PCA identifies the principal components by finding the eigenvectors of the covariance matrix of the data.\n",
    "Data points are then projected onto these principal components to reduce the dimensionality while retaining as much variance (information) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6c56e-1fc6-41a0-8520-c8d67ab6a5b6",
   "metadata": {},
   "source": [
    "# Q.2\n",
    "\n",
    "### Optimization Problem in PCA:\n",
    "\n",
    "The goal of PCA is to find a set of orthogonal axes (principal components) that maximize the variance of the projected data.\n",
    "Mathematically, PCA solves the optimization problem of finding the eigenvectors and eigenvalues of the covariance matrix of the data.\n",
    "The principal components are the eigenvectors corresponding to the largest eigenvalues, and the optimization problem aims to maximize these eigenvalues.\n",
    "\n",
    "### What It Achieves:\n",
    "\n",
    "It achieves dimensionality reduction by projecting the data onto the principal components with the highest variance, thereby retaining the most significant features of the data while reducing noise and redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e8b9a-32dd-46f6-a3ea-83d3c9b406e2",
   "metadata": {},
   "source": [
    "# Q.3 \n",
    "Covariance Matrix in PCA:\n",
    "\n",
    "The covariance matrix captures the pairwise covariances between the features of the data.\n",
    "PCA uses the covariance matrix to understand how the features vary with respect to each other.\n",
    "\n",
    "Relationship:\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the directions of maximum variance (principal components).\n",
    "The eigenvalues represent the amount of variance captured by each principal component.\n",
    "By performing an eigenvalue decomposition on the covariance matrix, PCA identifies the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8a6b6-5b20-4282-8403-0ed80b79ffbb",
   "metadata": {},
   "source": [
    "# Q.4 \n",
    "\n",
    "### Impact of Number of Principal Components:\n",
    "\n",
    "* Too Few Components: May result in loss of significant information, leading to underfitting and poor performance.\n",
    "* Too Many Components: May retain too much noise and redundancy, reducing the benefits of dimensionality reduction.\n",
    "* Optimal Number: Balances the trade-off between retaining significant variance and reducing dimensionality, leading to improved performance and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fa507-05c1-477e-9335-a4e007253cf6",
   "metadata": {},
   "source": [
    "# Q.5\n",
    "\n",
    "PCA in Feature Selection:\n",
    "\n",
    "PCA transforms the original features into a new set of features (principal components) ranked by the amount of variance they explain.\n",
    "By selecting the top principal components, PCA effectively reduces the feature set to the most informative components.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Dimensionality Reduction: Reduces the number of features, making models less complex and faster to train.\n",
    "Noise Reduction: Removes less informative and noisy features, improving model performance.\n",
    "Improved Interpretability: Simplifies the data, making it easier to visualize and understand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b73b2-e28f-4ae6-be3b-c42e3923cbc6",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "### common applications\n",
    "\n",
    "1. Data Visualization: Reduces high-dimensional data to 2 or 3 dimensions for visualization.\n",
    "2. Noise Reduction: Removes noise and redundant information from data.\n",
    "3. Feature Extraction: Identifies the most important features for model building.\n",
    "4. Image Compression: Reduces the dimensionality of image data while retaining essential features.\n",
    "5. Preprocessing Step: Serves as a preprocessing step for other machine learning algorithms to improve performance and reduce computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2ab-5e5d-4f43-a0c2-f1fb15c125ec",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "Relationship:\n",
    "\n",
    "Variance: Measures the spread of the data points in a particular direction. It is a statistical measure of how much the data points deviate from the mean.\n",
    "Spread: Refers to the extent of the data distribution in a given direction.\n",
    "In PCA, the principal components are the directions with the highest variance (spread), meaning they capture the most significant patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea307b67-fc0c-4f03-b0ab-c445ad3bc8ad",
   "metadata": {},
   "source": [
    "# Q.8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
