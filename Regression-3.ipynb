{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e8be2b-4524-410e-9aa9-c7b5560c099a",
   "metadata": {},
   "source": [
    "## Q.1  \n",
    "\n",
    "Ridge Regression- \n",
    "Ridge Regression, also known as Tikhonov regularization, is a technique used to address some of the limitations of Ordinary Least Squares (OLS) regression, particularly when dealing with multicollinearity (highly correlated independent variables) or when the number of predictors exceeds the number of observations. It achieves this by introducing a penalty term to the regression equation, which helps to shrink the coefficient estimates and prevent overfitting.\n",
    "\n",
    "Difference between ridge and ols - \n",
    "\n",
    "OLS: No regularization. The model focuses solely on minimizing the sum of squared residuals.\n",
    "\n",
    "Ridge Regression: Introduces a penalty term to the objective function, which helps to shrink the coefficients and mitigate the risk of overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466ff8b-19f6-4385-af9c-352c863163d9",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "\n",
    "Linearity:\n",
    "\n",
    "The relationship between the dependent variable and the independent variables is assumed to be linear. Ridge regression models the data as a linear combination of the input features.\n",
    "Independence:\n",
    "\n",
    "The observations are assumed to be independent of each other. This means that the error terms (residuals) should not be correlated across observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44853a-1963-476a-976b-663d0f3b468f",
   "metadata": {},
   "source": [
    "## Q.3\n",
    "\n",
    "Selecting the value of the tuning parameter (ùúÜ) in Ridge Regression is crucial because it determines the strength of the regularization applied to the model. A proper choice of ùúÜ balances bias and variance, improving the model's predictive performance. The most common method for selecting ùúÜ is through \n",
    "### cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a703f-984f-46b1-b201-12e8a2634e48",
   "metadata": {},
   "source": [
    "## Q.4\n",
    "\n",
    "Ridge Regression is not typically used for feature selection because it tends to shrink coefficients toward zero but does not force any of them to be exactly zero. As a result, Ridge Regression does not directly eliminate any features from the model. However, it can still be helpful in situations where feature selection is desirable, but it requires a different approach or a combination with other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d77a0-3823-428e-8911-33bdaeb9e5c8",
   "metadata": {},
   "source": [
    "## Q.5 \n",
    "\n",
    "Ridge Regression performs well in the presence of multicollinearity, which is a situation where two or more predictor variables in a regression model are highly correlated. This high correlation causes problems for Ordinary Least Squares (OLS) regression because it leads to large variances in the coefficient estimates, making them unstable and unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4c20f-e1dd-47cc-85b4-dbe984f85b5b",
   "metadata": {},
   "source": [
    "## Q.6\n",
    "\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909a720-931f-4817-b00c-c97fc84d3cad",
   "metadata": {},
   "source": [
    "## Q.7\n",
    "interpretation of  coefficients of Ridge Regression\n",
    "\n",
    "Suppose we have a Ridge Regression model predicting house prices based on various features.\n",
    "If the coefficient for the \"number of bedrooms\" is 10, it means that, all else being equal, each additional bedroom is associated with a $10 increase in house price.\n",
    "\n",
    "If the coefficient for the \"square footage\" is 0.05, it means that, all else being equal, each additional square foot is associated with a $0.05 increase in house price.\n",
    "\n",
    "The regularization in Ridge Regression means that even if a predictor variable is not highly correlated with the target variable, it might still have a non-zero coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1a4dd-7719-499a-87e2-37b2a7f14966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q.8\n",
    "\n",
    "Yes, Ridge Regression can be used for time-series data analysis, particularly in scenarios where the traditional linear regression assumptions may not hold, such as when dealing with multicollinearity or high dimensionality. However, using Ridge Regression for time-series data requires some adaptations and considerations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
