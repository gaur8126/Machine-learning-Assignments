{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9255723e-8fdf-4699-ba80-9390c7307a4b",
   "metadata": {},
   "source": [
    "# Q.1\n",
    "\n",
    "A Random Forest Regressor is an ensemble learning method that combines multiple decision tree regressors to improve predictive performance and reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6cbe7-3e2a-44cc-88d8-1575a52fd802",
   "metadata": {},
   "source": [
    "# Q.2\n",
    "\n",
    "Random Forest Regressor reduces the risk of overfitting through several mechanisms:\n",
    "\n",
    "* Bootstrap Sampling: Each tree in the forest is trained on a different bootstrap sample of the training data (sampling with replacement), ensuring diversity among the trees.\n",
    "* Feature Randomness: At each split in a tree, only a random subset of features is considered. This reduces the correlation between the trees and ensures that the trees capture different aspects of the data.\n",
    "*  Averaging Predictions: By averaging the predictions of multiple trees, random forest smooths out the predictions and reduces the model variance, leading to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238557f2-a400-44b5-9a02-09238bef4208",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "For regression tasks, the Random Forest Regressor aggregates the predictions of multiple decision trees by averaging their outputs. Specifically, for a given input, each tree in the forest produces a prediction, and the final output of the Random Forest Regressor is the mean of all these individual tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdebbeb-7d65-42f3-83ea-6c3bed29fd16",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Key hyperparameters of the Random Forest Regressor include:\n",
    "\n",
    "* n_estimators: The number of trees in the forest.\n",
    "* max_depth: The maximum depth of each tree.\n",
    "* min_samples_split: The minimum number of samples required to split an internal node.\n",
    "* min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "* max_features: The number of features to consider when looking for the best split.\n",
    "*  bootstrap: Whether bootstrap samples are used when building trees.\n",
    "* random_state: A seed for the random number generator to ensure reproducibility.\n",
    "* max_leaf_nodes: The maximum number of leaf nodes per tree.\n",
    "* n_jobs: The number of jobs to run in parallel for both fit and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc173d2-50c0-43bb-9382-533abe52f2b6",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "#### Decision Tree Regressor:\n",
    "\n",
    "Uses a single decision tree to model the relationship between features and target variable.\n",
    "Prone to overfitting, especially with deep trees, as it tries to capture all variations in the training data.\n",
    "\n",
    "### Random Forest Regressor:\n",
    "\n",
    "Combines multiple decision trees (a forest) to create an ensemble model.\n",
    "Reduces overfitting by averaging the predictions of many trees, each trained on different bootstrap samples and considering different feature subsets.\n",
    "Generally provides better generalization performance compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb79e9-cb71-4381-af78-1110b07e9a06",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "### Advantages:\n",
    "\n",
    "* Improved Accuracy: Typically provides better predictive performance compared to a single decision tree due to reduced variance.\n",
    "* Robustness: Less prone to overfitting due to averaging over many trees.\n",
    "* Versatility: Can handle both classification and regression tasks.\n",
    "* Feature Importance: Can provide estimates of feature importance, helping in feature selection.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "* Computationally Intensive: Requires more computational resources and memory, especially with a large number of trees and features.\n",
    "* Less Interpretability: The model is more complex and harder to interpret compared to a single decision tree.\n",
    "* Slower Predictions: Making predictions can be slower as it involves aggregating results from multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58103f-e735-4f56-9315-da44ee389743",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "The output of a Random Forest Regressor is a continuous numerical value, representing the average prediction from all the trees in the forest for a given input. This output is used to estimate the target variable in regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7d199-668f-4bca-83fa-2e7ba9e4ec2f",
   "metadata": {},
   "source": [
    "# Q.8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
